{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5ba1a9",
   "metadata": {},
   "source": [
    "# Create and Train our model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ab200",
   "metadata": {},
   "source": [
    "The following steps are:\n",
    "    -Create csv file \"data_CSVFile.csv\"\n",
    "    - Load Data and normalize images \n",
    "    - split data into 80% training and 20% testing \n",
    "    - Encode labels \n",
    "    - Build a CNN architecture with multi_output\n",
    "    - Choose model compilation parameters(optimizer, lossFunction,metrics,..)\n",
    "    - Fit the model with Nb_epochs and a defined batch size,..\n",
    "    - save the model \n",
    "    - Display histograms\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all we have to import necessary librairies\n",
    "from config import *\n",
    "#Config is our configuration file \n",
    "import resize_img_bb\n",
    "from keras .layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input \n",
    "from keras.layers import Convolution2D as conv\n",
    "from keras.layers import MaxPooling2D as maxpool\n",
    "from keras.layers import Flatten as flt\n",
    "from keras.models import Model \n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.utils import to_categorical \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pickle \n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \n",
    "    print(\"[INFO] resizing images and extracting bounding boxes...\")\n",
    "    \n",
    "    csvFile = resize_img_bb.annotations_to_csvFile(ANNOTS_PATH, IMAGES_PATH, WRITE_NEW_IMAGES_PATH,\n",
    "                                                   CLASSES_DICT, IMAGE_SIZE)\n",
    "    \n",
    "    print(\"[INFO]loading dataset...\")\n",
    "    data =[]\n",
    "    lables = []\n",
    "    bboxs=[]\n",
    "    img_paths=[]\n",
    "    dataset = pd.read_csv(csvFile)\n",
    "    \n",
    "    for index,row in dataset.iterrows():\n",
    "        image = cv2.imread(row[\"new_path\"])\n",
    "        coordinates= row['new_bb'][1:-1].split(',')\n",
    "        Xmin = float(coordinates[0])\n",
    "        Ymin = float(coordinates[1])\n",
    "        Xmax = float(coordinates[2])\n",
    "        Ymax = float(coordinates[3])\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)\n",
    "        labels.append(row[\"class\"])\n",
    "        bboxs.append((Xmin,Ymin,Xmax,Ymax))\n",
    "        img_paths.append(row[\"new_path\"])\n",
    "    data= np.array(data,stype =\"float32\")/255.0\n",
    "    labels = np.array(labels)\n",
    "    bboxes = np.array(bboxs,dtype=\"float32\")\n",
    "    img_paths = np.array(img_paths)\n",
    "    lb = LabelBinarizer()\n",
    "    labels = lb.fit_transform(labels)\n",
    "    \n",
    "    split = train_test_split(data, labels,bboxes, filenames, test_size=0.20,random_state=42)\n",
    "    (train_images,test_images) = split[:,:2]\n",
    "    (train_labels,test_labels) = split[:,2:4]\n",
    "    (train_bboxes,test_bboxes) =split[:,4:6]\n",
    "    (train_filenames,test_filenames)=split[:,6:]\n",
    "    if(lb.classes_ == 2):\n",
    "        labels = to_categorical(labels)\n",
    "    return (data,bboxes,img_paths,labels,len(lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b8a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MultiOutput_Model(nbClasses,img_size):\n",
    "    print(\"[INFO] constructing model architecture...\")\n",
    "    print(\"[INFO] input shape : (\"+str(img_size)+\",\"+str(img_size)+\",3)...\")\n",
    "    \n",
    "    model = conv(64,(3,3),input_shape=(img_size,img_size,3),activation='relu',padding = 'same', strides=(1,1))\n",
    "    model = conv(64,(3,3),activation='relu',padding = 'same', strides=(1,1)) (model)\n",
    "    model = maxpool(pool_size=(2,2),strides=(2,2),padding='same')(model)\n",
    "    print(\"2 x CONV  of 64 filters  and (3,3) kernel size...\")\n",
    "    print(\"Max pool of (2,2) kernel size...\")\n",
    "    model = conv(128,(3,3),activation='relu',padding = 'same', strides=(1,1)) (model)\n",
    "    model = maxpool(pool_size=(2,2),strides=(2,2),padding='same')(model)\n",
    "    print(\"CONV  of 128 filters  and (3,3) kernel size...\")\n",
    "    print(\"Max pool of (2,2) kernel size...\")\n",
    "    model = conv(256,(3,3),activation='relu',padding = 'same', strides=(1,1)) (model)\n",
    "    model = conv(256,(3,3),activation='relu',padding = 'same', strides=(1,1)) (model)\n",
    "    model = conv(256,(3,3),activation='relu',padding = 'same', strides=(1,1)) (model)\n",
    "    model = maxpool(pool_size=(2,2),strides=(2,2),padding='same')(model)\n",
    "    print(\"3 x CONV  of 256 filters  and (3,3) kernel size...\")\n",
    "    print(\"Max pool of (2,2) kernel size...\")\n",
    "    model = flt()(model)\n",
    "    print(\"[INFO] flatten the previous layer...\")\n",
    "    \n",
    "    print(\"[INFO] constructing head layer to ouput the predicted bounding box coordinates...\")\n",
    "    \n",
    "    bboxhead = Dense(128,activation = \"relu\")(model)\n",
    "    softmaxhead = Dropout(0.5) (softmaxhead)\n",
    "    bboxhead = Dense(64,activation=\"relu\")(bboxhead)\n",
    "    bboxhead = Dense(32,activation=\"relu\")(bboxhead)\n",
    "    print(\"[INFO] 3 X FoolConnected Layers 128-64-32 \")\n",
    "    bboxhead = Dense(4,activation=\"sigmoid\",name=\"bounding box\")(bboxhead)\n",
    "    print(\"[INFO] output bounding box Layer of 4 nodes(xmin,ymin,xmax,ymax)\")\n",
    "    \n",
    "    print(\"[INFO] constructing head layer to ouput the predicted class label...\")\n",
    "    \n",
    "    softmaxhead = Dense(512,activation=\"relu\")(model)\n",
    "    softmaxhead = Dropout(0.5) (softmaxhead)\n",
    "    print(\"[INFO] FoolConnected Layer of 512 nodes + Dropout of 0.5 \")\n",
    "    softmaxhead = Dense(256,activation=\"relu\")(softmaxhead)\n",
    "    softmaxhead = Dropout(0.5) (softmaxhead)\n",
    "    print(\"[INFO] FoolConnected Layer of 256 nodes + Dropout of 0.5 \")\n",
    "    softmaxhead = Dense(nbClasses,activation=\"sotfmax\",name=\"class_label\")(softmaxhead)\n",
    "    print(\"[INFO] output class label Layer of \"+str(nbClasses)+\" nodes \")\n",
    "    \n",
    "    FinalModel = Model(inputs = model,outputs = (bboxhead,softmaxhead))\n",
    "    return FinalModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_Model(model):\n",
    "    losses={'class_label': 'categorical_crossentropy',\n",
    "            'bounding box': 'mean_squared_error'}\n",
    "    lossweights={'class_label': 1.0,\n",
    "            'bounding box': 1.0}\n",
    "    opt = Adam(lr=INIT_LR)\n",
    "    model.compile(loss = losses,optimizer=opt,metrics=[\"accuracy\"],loss_weights=lossweights)\n",
    "    print(model.compile)\n",
    "    return model\n",
    "\n",
    "\n",
    "def Fit_Model(model,trainImages,testImages,Trainlabels,Testlabels,TrainBboxs,TestBboxs):\n",
    "    trainTargets = {\"class_label\": Trainlabels,\n",
    "                \"bounding box\": TrainBboxs}\n",
    "    testTargets = {\"class_label\": Testlabels,\n",
    "                \"bounding box\": TestBboxs}\n",
    "    print(\"[INFO] training model...\")    \n",
    "    H = model.fit(trainImages,trainTargets,validation_data=(testImages,testTargets),batch_size=BATCH_SIZE,epochs=NUM_EPOCHS,verbose =1)\n",
    "    #verbose =1 => will show you an animated progress bar: \n",
    "    # [==============]\n",
    "    #verbose = 0  => will show you nothing (silent)\n",
    "    #verbose = 2 => will just mention the number of epoch :\n",
    "    # Epoch 1/10\n",
    "    print(\"[INFO] saving object detector model...\")\n",
    "    model.save(MODEL_PATH,save_format=\"h5\")\n",
    "    \n",
    "    Loss_names = ['loss','class_label_loss','bounding box_loss']\n",
    "    N= np.arrange(NUM_EPOCHS)\n",
    "    plt.style.use(\"ggplot\")\n",
    "    (fig,ax)=plt.subplots(3,1,figsize=(13,13))\n",
    "    for(i,l) in enumerate(Loss_names):\n",
    "        title = \"Loss for {}\".format(l) if l != \"loss\" else\"Total loss\"\n",
    "        ax[i].set_title(title)\n",
    "        ax[i].set_xlabel(\"Epoch\")\n",
    "        ax[i].set_ylabel(\"loss\")\n",
    "        ax[i].plot(N,H.history[l],label = l)\n",
    "        ax[i].plot(N,H.history[\"val_\"+l],label = \"val_\"+l)\n",
    "        ax[i].legend()\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(PLOTS_PATH, \"losses.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(N, H.history[\"class_label_accuracy\"],label=\"class_label_train_acc\")\n",
    "    return H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
